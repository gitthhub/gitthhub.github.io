---
layout:     post
title:      CNN Model - NIN
subtitle:   论文分析
date:       2019-03-22
author:     vhpg
header-img: img/placeholder_img.png
catalog: true
tags:
    - Deep Learning
    - CNN Model
---
> 本篇文章观点仅限于目前的理解，后续若有新的理解，还会继续更新。

#### 1. Introduce
  NIN(Network In Network)是NUS(National University of Singapore)于2014年发表在ICLR上的一篇文章中提出的，作者首先分析了传统的CNN网络的一些问题，并针对这些问题，提出了自己的改进方法，并将网络结构命名为NIN。

#### 2. Convolutional Neural Networks
  对传统的CNN网络，没有经过非线性激活函数之前的卷积操作，实际上只是一个线性操作，如果卷积结果为正，这样经过一个ReLU函数后没有影响，就相当于是一个线性卷积(对前一层receptive field的线性编码)。

  作者认为，对于`latent concepts`为线性可分的问题，这种线性的抽象能力已经足够，但一个好的抽象表达往往是与输入数据呈高度非线性的。
  `Representations that achieve good abstraction are generally highly nonlinear functions of the input data.`

  在CNN网络中，在每一层使用较多的卷积核以对各种`latent concepts`分类，在一定程度上可以弥补网络线性抽象能力差的问题，但这样会给下一层网络带来较大的输入，网络的整体参数数量就会增加。

  所以作者考虑在这里做出改进，在每一层进行卷积操作后，并不急着把feature map传入下一层，而是对feature map进行更多非线性的操作，以提取出抽象能力更强的特征。
  ![2019-03-22_142056](/assets/2019-03-22_142056.png)

#### 3. Network In Network
  NIN网络创新点主要包括两个方面，分别在下面进行阐述。

##### 3.1 MLP Convolution Layers
  由于传统的多层感知机网络具有较强的非线性抽象能力，所以作者考虑将经过卷积操作得到的feature map输入到一个MLP网络中，以提高其非线性抽象能力。

  在多通道的feature map上进行该操作，就相当于是将各通道的信息进行混合，此时每个全连接层就相当于对feature map进行`1*1`的卷积操作。

  作者把这样的**传统conv+1x1conv+1x1conv**操作封装成一个子块，由这样的子块堆叠而成的网络就是NIN网络，如下图所示：
  ![2019-03-22_142852](/assets/2019-03-22_142852.png)

  从目前的应用来看，**1*1 Convolution**效果确实非常好，此后的GoogLeNet和ResNet都借鉴了这种操作。
  **1*1 Convolution**好用的原因应该有以下几点：
  * 融合feature map各通道的信息，提高了网络的抽象能力，进而提高了网络的泛化性能；
  * 可以实现对feature map层的压缩，以降低参数数量，进而可以提高网络层数；

##### 3.2 Global Average Pooling
  对传统的CNN网络分类器结构通常是：卷积层作为特征提取器，最后一层卷积输出的结果展开为向量，连接至全连接层进行分类，全连接层的输出再输入到softmax层作为最终的分类结果。

  作者认为这样的操作可解释性不强，并且全连接层容易引起网络的过拟合，所以作者提出了一种全局均值池化的方法，对最后一个卷积层的输出，各通道的值直接求均值后，以该值作为softmax层的输入以进行分类，具体如下图所示：
  ![global_pooling](/assets/global_pooling.png)

  **Global Average Pooling**的优点如下：
  * 不引入新的参数，避免了全连接层带来的参数数量增加和过拟合；
  * 增加网络的可解释性，输出的每个通道对应于一个类别；
  * 通过实验发现，全局均值池化还有正则化的作用。

##### 3.3 Overall Structure
  论文中给出的网络结构如下图所示，在除最后一个block外的每一个NIN block后面，作者还使用了一个Max Pooling和Dropout层，最后一个block的输出channel数要等于类别数，以进行全局均值池化。
  ![2019-03-22_143428](/assets/2019-03-22_143428.png)

#### 4. Experiments
  论文中作者使用了`CIFAR-10/CIFAR-100/SVHN/MNIST`这几个小数据集进行网络性能的分析，可以借鉴。

#### Reference
[Network In Network](https://arxiv.org/abs/1312.4400)
[Blog 1](http://teleported.in/posts/network-in-network/)
[Blog 2](https://zhuanlan.zhihu.com/p/37683646)
[3](http://gluon.ai/chapter_convolutional-neural-networks/nin.html)

---
layout:     post
title:      CNN Model - DenseNet
subtitle:   论文分析
date:       2019-04-16
author:     vhpg
header-img: img/placeholder_img.png
catalog: true
tags:
    - Deep Learning
    - CNN Model
---
> 本篇文章观点仅限于目前的理解，后续若有新的理解，还会继续更新。

#### 1. Introduce
DenseNet于2017年CVPR上的一篇论文中提出，该网络是对ResNet等跳层连接的网络的进一步发展，作者提出了一种稠密残差连接的模块，并据此构建出较深的网络结构，取得了不错的效果。

如下图所示，为一个稠密残差连接的模块的示意图，在图中，X_0为输入，其后各层为卷积后的特征图：
该模块的特点为：
* 各个卷积层的输入为前面各个卷积层输出结果的累加(沿channel方向连接);
* 各个卷积层使用`1*1`的卷积核进行维度压缩，输出控制为相同shape(`w*h*c`);
![Selection_001](/assets/Selection_001_9gpnyxfdx.png)

使用该模块够层的卷积网络的示意图如下，网络中，首先使用一个较大的卷积核(如`7*7`)和步长对图像进行卷积，提取出一个较小尺寸和多通道的特征图，然后叠加`Dense Block`模块进行特征提取工作，模块之间由卷积和池化操作来减小特征的尺寸：
![Selection_002](/assets/Selection_002_zomg4s567.png)

#### 2. DenseNet Detials
> Basic DenseNet: DenseNet中使用的是一种预激活的结构，即BN-ReLU-ConV，如下图所示：

![Selection_001](/assets/Selection_001_un64nz2r3.png)

> DenseNet-B: 中使用了1*1的卷积层用于降维(Bottleneck Layer)，特征首先经过Bottlenec Layer输出channel数为4k，然后经过普通卷积输出channel为k，如下图所示：

![Selection_002](/assets/Selection_002_b7qizbd2a.png)

> DenseNet-C: 在DenseNet网络中，模块中的特征channel个数k是网络容量的一个重要指标，若前一层block中的channel为m，则我们可以设置block间的转移层输出的channel个数为theta*m，0<theta<=1，这样可通过theta参数控制网络规模。论文中将0<theta<1时的DenseNet记为DenseNet-C，将theta=0.5时的DenseNet即为DenseNet-BC。

#### 3. Advantages
> 梯度反传更为高效，可避免梯度消失问题，如下图所示：
![1_9atnQFu8ncrqFqZdB_LNVg](/assets/1_9atnQFu8ncrqFqZdB_LNVg.png)

> 如下图所示，相比于ResNet中相邻两层的channel数保持相同，在DenseNet中卷积的输出保持为k，相比之下大大降低了参数数量和计算复杂度：

![1_03pZkWqHN7A3pd81Pi-cIQ](/assets/1_03pZkWqHN7A3pd81Pi-cIQ.png)

#### 4. Conclusion
论文中作者分别使用不同深度(100层及以上)和不同尺度(k值)的网络在`CIFAR-10`/`CIFAR-100`/`SVHN`和`ImageNet`等分类数据集上都进行了实验，相比于之前的网络，都达到了当时最好的分类效果，这里不再赘述。

总的来说，DenseNet是对ResNet等残差连接网络的进一步发展，思路可以借鉴。

#### Reference
[DenseNet](https://arxiv.org/pdf/1608.06993.pdf)

[Blog 1](https://towardsdatascience.com/review-densenet-image-classification-b6631a8ef803)
